[{"title":"hadoop中yarn的一些配置","date":"2017-04-05T19:00:00.000Z","path":"2017/04/05/hadoop中yarn的一些配置/","text":"在Hadoop2中经常会出现Container is running beyond virtual memory limits的错误, 详细解答可以参考Stackoverflow. 具体做法我个人更倾向于cloudera的做法, 直接将yarn.nodemanager.vmem-check-enabled设置为false.yarn-site.xml1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;另外对mapreduce的配置, 如HDP在文章中用的Map task的container大小为4GB, Reduce task的container大小为8GB. cloudera建议这个比例为2/3.mapred-site.xml12345678&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;8192&lt;/value&gt;&lt;/property&gt;上述是container的使用的内存大小, 在实际使用时默认是把container内存的0.8分配给task JVM的堆. 当然这个也可以手动配置, 但其大小不能超过container的内存大小, 如下.mapred-site.xml12345678&lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx3072m&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt; &lt;value&gt;-Xmx6144m&lt;/value&gt;&lt;/property&gt;","tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://yoursite.com/tags/bigdata/"},{"name":"配置","slug":"配置","permalink":"http://yoursite.com/tags/配置/"}]},{"title":"在JupyterHub中配置pyspark","date":"2017-03-05T19:00:00.000Z","path":"2017/03/05/在JupyterHub中配置pyspark/","text":"Spark是一个常用的大数据处理工具, Jupyter是一个notebook的工具, 在Jupyter中可以安装一些kernel进行交互式操作, 具体kernel的安装方法可以参考其官方文档. JupyterHub是一个Jupyter多租户的管理工具. 在本文中主要是介绍如何在Jupyter中配置一个pyspark核. 在图1.JupyterHub和pyspark的网络拓扑图中, JupyterHub作为spark任务的提交节点, spark01-spark04为一个简单的Spark集群. 图1. JupyterHub和pyspark的网络拓扑图假设在JupyterHub这个节点上已安装好JupyterHub, 其路径为/opt/jupyterhub. 在spark01-spark04这个四个节点上分别安装python, 假设其路径为/opt/python. 另外, 在JupyterHub和spark01-spark04这五个节点上的Spark安装路径为/opt/spark. 那么其对应的配置文件为:JupyterHub:/opt/jupyterhub/share/jupyter/kernels/pyspark/kernel.json12345678910111213141516171819// 连接YARN集群&#123; \"display_name\": \"pyspark\", \"language\": \"python\", \"argv\": [ \"/opt/python/bin/python\", \"-m\", \"ipykernel\", \"-f\", \"&#123;connect_file&#125;\" ], \"env\": &#123; \"PYSPARK_PYTHON\": \"/opt/python/bin/python\", \"SPARK_HOME\": \"/opt/spark/python\", \"PYTHONPATH\": \"/opt/spark/python/lib/py4j-0.9-src.zip:/opt/spark/python\", \"PYTHONSTARTUP\": \"/opt/spark/python/pyspark/shell.py\", \"PYSPARK_SUBMIT_ARGS\": \"--master yarn-client pyspark-shell\" &#125;&#125;如果使用的是Standalone方式, 则将第17行改为:“PYSPARK_SUBMIT_ARGS”: “–master spark://sparkMaster:7077 pyspark-shell”.","tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://yoursite.com/tags/bigdata/"},{"name":"配置","slug":"配置","permalink":"http://yoursite.com/tags/配置/"}]},{"title":"开篇","date":"2017-02-25T19:00:00.000Z","path":"2017/02/25/开篇/","text":"在2015年国庆节的时候就想搭一个博客, 一来用于记录自己的一些生活体会, 另一个主要用于对自己所学的东西做个笔记, 但是由于自己当时的懒惰, 再加之自己技术不成熟(主要还是自己懒惰)等一些原因一直未能完成. 当时我对Python比较熟悉, 但是想多扩大一下自己的眼界^_^, 不想用Pelican. 于是就尝试用Jekyll去搭建博客, 当时被各种模板主题之类的弄得晕头转向, 结果就一直拖着. 直到去年的时候又铆劲要搭博客, 当时觉得如果再用Jekyll就没有新鲜感了, 于是重新找新的生成器. 找来找去找了自己不熟悉的Hexo, 个中原因是被Hexo超快的生成速度所吸引(其实我也没看出来-_-). 当时也差点被Hexo的各种主题之类的弄晕, 最后还是决定先选一个主题搭起来再说. 一番艰难的抉择之后选定了Yilia, 主要是看见好几个博客用这个主题来介绍如何搭建博客的, 我想这个主题应该比较适合新手. 选好生成器和主题后, 几经折腾, 博客已初步搭建完成. 完成后这个博客将首先用来记录自己的一些成长(虽然自己已不年轻), 如生活体会, 一些技术总结之类的; 其次自己要努力做到每周都有内容更新. 从自己搭建博客这个过程, 自己也得到一个重要的教训: 有想法一定要行动起来, 不然始终只会是一个想法. 用古人的话来说就是: 道虽通不行不至, 事虽小不为不成.","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]}]